{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b87fcd5",
   "metadata": {},
   "source": [
    "## Neural Network for Number Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce92fc5",
   "metadata": {},
   "source": [
    "### Downloading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "706be826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7285df",
   "metadata": {},
   "source": [
    "### For GPU Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eec5b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')            #cuda is a parallel processing platform for gpus, if available utilize that for faster comp\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "device = get_default_device()\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    # Move tensor to chosen device\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]    #list or tuple\n",
    "    return data.to(device, non_blocking=True)          #for tensors or models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45f77a",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "480d4d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor()) #MNIST class is used as a constructor and data is conv to tensor\n",
    "print(len(dataset))\n",
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())\n",
    "print(len(test_dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae7518",
   "metadata": {},
   "source": [
    "### Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bdd58235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 128\n",
    "validation_size = 10000\n",
    "train_size = len(dataset) - validation_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, validation_size])  #randomly splits the dataset so that validation isn't just the last 10k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8f7f1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    #wrap a dataloader to move data to a device\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # yeilds the next batch of data after moving it to device\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)   # yeild is used to create a generator function which can be iterated in a for loop\n",
    "\n",
    "    def __len__(self):\n",
    "        #number of batches\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6ec412b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DeviceDataLoader(DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True), device)\n",
    "val_loader = DeviceDataLoader(DataLoader(val_ds, batch_size=batch_size*2, num_workers=4, pin_memory=True), device)\n",
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=batch_size*2, num_workers=4, pin_memory=True), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f814bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 16\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a708236",
   "metadata": {},
   "source": [
    "### Defining Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c7eb513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)                                   # returns max value and index\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))    # if preds = labels returns 1 nai to 0, then takes mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe803b2",
   "metadata": {},
   "source": [
    "### Extending nn.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4d943005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super().__init__()\n",
    "        self.Sequential = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),              # we have two hidden layers and two non-linear activation functions b/w them\n",
    "            nn.ReLU(),                                          # non-linearity helps in learning complex patterns\n",
    "            nn.Linear(hidden_size2, num_classes),)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(xb.size(0), -1)         # flatten inputs from 3d (greyscale and 28x28) to 1d as wohi model mai input jaata hai.\n",
    "        outputs = self.Sequential(xb)        # xb.size(0) is the batch size and -1 is used get other dim and make it generalized\n",
    "        return outputs                       \n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)                    # gen preds\n",
    "        loss = F.cross_entropy(outputs, labels)   # calc loss by converting preds into softmax prob and takeing neg log \n",
    "        return loss                               # its is used as it focuses on maximizing the prob of the correct class\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)                                 # gen preds\n",
    "        loss = F.cross_entropy(outputs, labels)                # calc loss\n",
    "        acc = accuracy(outputs, labels)                        # calc acc\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]           # extracts all the validation losses\n",
    "        epoch_loss = torch.stack(batch_losses).mean()             # combines all those scalars into a single tensor and computes the average\n",
    "        batch_accs = [x['val_acc'] for x in outputs]              # extracts all the validation accuracies\n",
    "        epoch_acc = torch.stack(batch_accs).mean()                # combine accuracies then av\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}%\".format(epoch+1, result['val_loss'], result['val_acc']*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c43ad",
   "metadata": {},
   "source": [
    "Defining Model and Loading the Weights and Parameters of the model which we already trained (if not then run the training cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52e1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNIST(\n",
       "  (Sequential): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MNIST(input_size, hidden_size1, hidden_size2, num_classes)     # create model\n",
    "to_device(model, device)                                               # move model to device\n",
    "#model.load_state_dict(torch.load('mnist_weights.pth'))   # load the pretrained weights if .pth is available else leave commented and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69112db",
   "metadata": {},
   "source": [
    "### Defining the evaluate and fit func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "80f13442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    #Evaluates model on validation set and returns loss and accuracy\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]     \n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_fn = torch.optim.SGD):\n",
    "    #Trains model using gradient descent\n",
    "    optimizer = opt_fn(model.parameters(), lr)   # model.parameter contains all the weights and biases of the model and is passed to optimizer\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #train\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()                     # update weights\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        #validate\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3bdeb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d5edf6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial result: loss = 2.3120, acc = 11.2695%\n",
      "Epoch [1], val_loss: 0.3483, val_acc: 89.3750%\n",
      "Epoch [2], val_loss: 0.2550, val_acc: 92.6270%\n",
      "Epoch [3], val_loss: 0.2454, val_acc: 92.9102%\n",
      "Epoch [4], val_loss: 0.1764, val_acc: 95.1758%\n",
      "Epoch [5], val_loss: 0.1685, val_acc: 95.1758%\n",
      "Epoch [6], val_loss: 0.1577, val_acc: 95.6738%\n",
      "Epoch [7], val_loss: 0.1327, val_acc: 96.1328%\n",
      "Epoch [8], val_loss: 0.1212, val_acc: 96.4844%\n",
      "Epoch [9], val_loss: 0.1285, val_acc: 96.1133%\n",
      "Epoch [10], val_loss: 0.1079, val_acc: 96.9727%\n",
      "[{'val_loss': 0.34826353192329407, 'val_acc': 0.893750011920929}, {'val_loss': 0.2550021708011627, 'val_acc': 0.92626953125}, {'val_loss': 0.24542582035064697, 'val_acc': 0.9291015863418579}, {'val_loss': 0.1763567328453064, 'val_acc': 0.9517577886581421}, {'val_loss': 0.16853861510753632, 'val_acc': 0.9517577886581421}, {'val_loss': 0.1576763391494751, 'val_acc': 0.956738293170929}, {'val_loss': 0.1326693445444107, 'val_acc': 0.9613281488418579}, {'val_loss': 0.12115774303674698, 'val_acc': 0.96484375}, {'val_loss': 0.12849317491054535, 'val_acc': 0.961132824420929}, {'val_loss': 0.107865110039711, 'val_acc': 0.9697265625}]\n"
     ]
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)                     # gives initial loss and acc\n",
    "print(\"Initial result: loss = {:.4f}, acc = {:.4f}%\".format(result0['val_loss'], result0['val_acc']*100))\n",
    "\n",
    "history = fit(10, 0.1, model, train_loader, val_loader)    # trains the model and returns history of loss and acc through epochs\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4e545",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b21d7daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 2  Actual: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGblJREFUeJzt3Q1MVff9x/Hv9QG0Fi5DlAdBCz7UVSdmTimxZbYSKG2cqFnazmy6dBodNlVmu7GtWrc1TJtsTRdn12SRNqsPNRmaOkemqLAHqNHOMLPViWMDK2jrwuXBgQ7OP7/jH+ZVqD1X4HvvPe9X8sv13nO+3OPhcD73d87vnuOxLMsSAACG2LChfkMAAAwCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACpGSJDp7u6WixcvSlRUlHg8Hu3FAQA4ZK5v0NraKklJSTJs2LDQCSATPikpKdqLAQC4Sw0NDZKcnBw6h+BMzwcAEPrutD8ftADavn273HfffTJq1CjJyMiQEydOfKo6DrsBQHi40/58UAJo7969UlhYKJs3b5b3339f0tPTJTc3Vy5fvjwYbwcACEXWIJg3b55VUFDQ+7yrq8tKSkqyiouL71jr8/nM1blpNBqNJqHdzP78kwx4D+jatWty6tQpyc7O7n3NjIIwz6uqqm6bv7OzU1paWvwaACD8DXgAffzxx9LV1SXx8fF+r5vnTU1Nt81fXFwsXq+3tzECDgDcQX0UXFFRkfh8vt5mhu0BAMLfgH8PKC4uToYPHy6XLl3ye908T0hIuG3+yMhIuwEA3GXAe0AREREyZ84cKS8v97u6gXmemZk50G8HAAhRg3IlBDMEe8WKFfKFL3xB5s2bJ6+++qq0t7fL17/+9cF4OwBACBqUAHryySflo48+kk2bNtkDD2bPni1lZWW3DUwAALiXx4zFliBihmGb0XAAgNBmBpZFR0cH7yg4AIA7EUAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUjdN4WCE7Dhjn/TBYTEyNDYerUqY5rli5dGtB7ZWRkOK75/e9/77jm5ZdfdlzT0dHhuAbBiR4QAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFVyMFEEvkIt9Ll68OKD3ys7OdlyzfPnygN4r3GRlZTmuSU9Pd1zzpS99yXENghM9IACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACq4GCkCNn36dMc1zz33nOOaBx98cEgucomhx+/J3egBAQBUEEAAgPAIoJdeekk8Ho9fC+RQDQAgvA3KOaAZM2bIkSNH/vcmIzjVBADwNyjJYAInISFhMH40ACBMDMo5oHPnzklSUpKkpaXZtyuur6/vd97Ozk5paWnxawCA8DfgAZSRkSElJSVSVlYmO3bskLq6Onn44YeltbW1z/mLi4vF6/X2tpSUlIFeJACAGwIoLy9PvvzlL8usWbMkNzdXDh06JM3NzfLOO+/0OX9RUZH4fL7e1tDQMNCLBAAIQoM+OiAmJkamTZsmtbW1fU6PjIy0GwDAXQb9e0BtbW1y/vx5SUxMHOy3AgC4OYA2btwoFRUV8s9//lP+9Kc/yZIlS2T48OHy9NNPD/RbAQBC2IAfgrtw4YIdNleuXJFx48bJQw89JNXV1fa/AQDo4bEsy5IgYoZhm9FwCH5Hjx51XLNgwQIJN3/5y18c15w4ccJxza5du2SoLF261HFNQUGB4xrzNQynHn/8ccc1x44dc1yDu2cGlkVHR/c7nWvBAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQACM8b0iF8mdupOzV79uyAbmro1J49eyQQW7dudVwTyF18//3vf0swC+T3FIhAbkaZkJAwKMuCoUcPCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACggqthI2CHDx92XPPAAw84rhk5cqTjmg8//FAC0d3dHVAdAOfoAQEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFDBxUgxpJqamrQXAZ/Cjh07HNds3brVcc2IEeyC3IweEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABVcCRDAbdauXeu4hguLwil6QAAAFQQQACA0AqiyslIWLVokSUlJ4vF4ZP/+/X7TLcuSTZs2SWJioowePVqys7Pl3LlzA7nMAAA3BlB7e7ukp6fL9u3b+5y+bds2ee211+T111+X9957T8aMGSO5ubnS0dExEMsLAAgTjs8a5uXl2a0vpvfz6quvyve//31ZvHix/dpbb70l8fHxdk/pqaeeuvslBgCEhQE9B1RXV2ffctkcduvh9XolIyNDqqqq+qzp7OyUlpYWvwYACH8DGkAmfAzT47mZed4z7VbFxcV2SPW0lJSUgVwkAECQUh8FV1RUJD6fr7c1NDRoLxIAINQCKCEhwX68dOmS3+vmec+0W0VGRkp0dLRfAwCEvwENoNTUVDtoysvLe18z53TMaLjMzMyBfCsAgNtGwbW1tUltba3fwIPTp09LbGysTJw4UdavXy8/+tGPZOrUqXYgvfjii/Z3hvLz8wd62QEAbgqgkydPyiOPPNL7vLCw0H5csWKFlJSUyAsvvGB/V2j16tXS3NwsDz30kJSVlcmoUaMGdskBACHNY5kv7wQRc8jOjIYDoOfIkSOOax599FHHNf/9738d1/R8x9CJ3/72t45rcPfMwLJPOq+vPgoOAOBOBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAIDQuB0DgNCxcOHCgOrMbVSGQmNjo+MarmwdPugBAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUMHFSIEw9p3vfCeguoiICMc1XV1djmtefvllxzUIH/SAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqOBipECISElJcVzzwAMPyFDp6OhwXPPGG28MyrIgNNADAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIKLkQIKJkyY4Ljm0KFDjmsSExMd1wBDhR4QAEAFAQQACI0AqqyslEWLFklSUpJ4PB7Zv3+/3/SVK1far9/cHnvssYFcZgCAGwOovb1d0tPTZfv27f3OYwKnsbGxt+3evftulxMA4PZBCHl5eXb7JJGRkZKQkHA3ywUACHODcg7o+PHjMn78eLn//vtl7dq1cuXKlX7n7ezslJaWFr8GAAh/Ax5A5vDbW2+9JeXl5bJ161apqKiwe0xdXV19zl9cXCxer7e3BXLfewBA6PFYlmUFXOzxSGlpqeTn5/c7zz/+8Q+ZPHmyHDlyRBYuXNhnD8i0HqYHRAgh3AXyPaCysjLHNTNmzJChYs4POxUVFTUoy4Lg4PP5JDo6Wm8YdlpamsTFxUltbW2/54vMAt7cAADhb9AD6MKFC/Y5IL6RDQC4q1FwbW1tfr2Zuro6OX36tMTGxtpty5YtsmzZMnsU3Pnz5+WFF16QKVOmSG5urtO3AgCEMccBdPLkSXnkkUd6nxcWFtqPK1askB07dkhNTY28+eab0tzcbH9ZNScnR374wx/ah9oAABiQQQiDwQxCMKPhMHSGDx8eUF0gJ7h/85vfOK6Jj4+XcGMG8AzV72mo9DXI6E6OHTs2KMuC4KA+CAEAgL4QQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAELjdgwIPzffXsOJ3/3udwO+LG4RyNWwg+zC9bfJz893XPP3v//dcc2HH37ouAbBiR4QAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFR4ryK5w2NLSIl6vV3sxQtasWbMc1xw8eDCg90pOTg6oDujR3t7uuOarX/2q45r9+/c7rsHd8/l8Eh0d3e90ekAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUjNB5WwyWdevWOa7hoqI6F2l0qrCw0HFNXl6eBGL27NmOa6ZMmeK4ZsyYMY5r3nzzTcc1e/fulUBs3LgxoAsq49OhBwQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFFyMF7lJZWdmQXFj0gw8+cFyzc+dOCURCQoLjmqefftpxTUFBgeOatLQ0xzXf+MY3JBBxcXGOa772ta85rmlraxM3ogcEAFBBAAEAgj+AiouLZe7cuRIVFSXjx4+X/Px8OXv2rN88HR0ddrd67Nixcu+998qyZcvk0qVLA73cAAA3BVBFRYUdLtXV1XL48GG5fv265OTkSHt7e+88GzZskHfffVf27dtnz3/x4kVZunTpYCw7AMAtgxBuPdlaUlJi94ROnTolWVlZ9l0ef/nLX8quXbvk0Ucf7T0J+tnPftYOrQcffHBglx4A4M5zQD23FY6NjbUfTRCZXlF2dnbvPNOnT5eJEydKVVVVnz+js7PTvoXtzQ0AEP4CDqDu7m5Zv369zJ8/X2bOnGm/1tTUJBERERITE+M3b3x8vD2tv/NKXq+3t6WkpAS6SAAANwSQORd05swZ2bNnz10tQFFRkd2T6mkNDQ139fMAAGH8RdR169bJwYMHpbKyUpKTk/2+vHbt2jVpbm726wWZUXD9fbEtMjLSbgAAd3HUA7Isyw6f0tJSOXr0qKSmpvpNnzNnjowcOVLKy8t7XzPDtOvr6yUzM3PglhoA4K4ekDnsZka4HThwwP4uUM95HXPuZvTo0fbjM888Y19mxAxMiI6OlmeffdYOH0bAAQACDqAdO3bYjwsWLPB73Qy1Xrlypf3vn/70pzJs2DD7C6hmhFtubq78/Oc/d/I2AAAX8FjmuFoQMcOwTU8KgXnjjTeG7EKNwcyM0gzE8uXLHdccOnTIcU1ra6vjmnA0btw4xzWTJ092XPO9731PAvHEE084rjEfvp0qLS2VcGQGlpkjYf3hWnAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAgNC5IyqC15YtWxzXZGVlBfRe06ZNk6HQcxuQwV4PxuXLlwOqQ2A++uijIalZv369BOLMmTMB1eHToQcEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABAhceyLEuCSEtLi3i9Xu3FAADcJZ/PJ9HR0f1OpwcEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAIPgDqLi4WObOnStRUVEyfvx4yc/Pl7Nnz/rNs2DBAvF4PH5tzZo1A73cAAA3BVBFRYUUFBRIdXW1HD58WK5fvy45OTnS3t7uN9+qVauksbGxt23btm2glxsAEOJGOJm5rKzM73lJSYndEzp16pRkZWX1vn7PPfdIQkLCwC0lACDs3NU5IJ/PZz/Gxsb6vf72229LXFyczJw5U4qKiuTq1av9/ozOzk5paWnxawAAF7AC1NXVZT3xxBPW/Pnz/V7/xS9+YZWVlVk1NTXWr371K2vChAnWkiVL+v05mzdvtsxi0Gg0Gk3Cqvl8vk/MkYADaM2aNdakSZOshoaGT5yvvLzcXpDa2to+p3d0dNgL2dPMz9NeaTQajUaTQQ8gR+eAeqxbt04OHjwolZWVkpyc/InzZmRk2I+1tbUyefLk26ZHRkbaDQDgLo4CyPSYnn32WSktLZXjx49LamrqHWtOnz5tPyYmJga+lAAAdweQGYK9a9cuOXDggP1doKamJvt1r9cro0ePlvPnz9vTH3/8cRk7dqzU1NTIhg0b7BFys2bNGqz/AwAgFDk579Pfcb6dO3fa0+vr662srCwrNjbWioyMtKZMmWI9//zzdzwOeDMzr/ZxSxqNRqPJXbc77fs9/x8sQcMMwzY9KgBAaDNf1YmOju53OteCAwCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoCLoAsixLexEAAEOwPw+6AGptbdVeBADAEOzPPVaQdTm6u7vl4sWLEhUVJR6Px29aS0uLpKSkSENDg0RHR4tbsR5uYD3cwHq4gfUQPOvBxIoJn6SkJBk2rP9+zggJMmZhk5OTP3Ees1LdvIH1YD3cwHq4gfVwA+shONaD1+u94zxBdwgOAOAOBBAAQEVIBVBkZKRs3rzZfnQz1sMNrIcbWA83sB5Cbz0E3SAEAIA7hFQPCAAQPgggAIAKAggAoIIAAgCoCJkA2r59u9x3330yatQoycjIkBMnTojbvPTSS/bVIW5u06dPl3BXWVkpixYtsr9Vbf7P+/fv95tuxtFs2rRJEhMTZfTo0ZKdnS3nzp0Tt62HlStX3rZ9PPbYYxJOiouLZe7cufaVUsaPHy/5+fly9uxZv3k6OjqkoKBAxo4dK/fee68sW7ZMLl26JG5bDwsWLLhte1izZo0Ek5AIoL1790phYaE9tPD999+X9PR0yc3NlcuXL4vbzJgxQxobG3vbH/7wBwl37e3t9u/cfAjpy7Zt2+S1116T119/Xd577z0ZM2aMvX2YHZGb1oNhAufm7WP37t0STioqKuxwqa6ulsOHD8v169clJyfHXjc9NmzYIO+++67s27fPnt9c2mvp0qXitvVgrFq1ym97MH8rQcUKAfPmzbMKCgp6n3d1dVlJSUlWcXGx5SabN2+20tPTLTczm2xpaWnv8+7ubishIcF65ZVXel9rbm62IiMjrd27d1tuWQ/GihUrrMWLF1tucvnyZXtdVFRU9P7uR44cae3bt693nr/97W/2PFVVVZZb1oPxxS9+0XruueesYBb0PaBr167JqVOn7MMqN18vzjyvqqoStzGHlswhmLS0NFm+fLnU19eLm9XV1UlTU5Pf9mGuQWUO07px+zh+/Lh9SOb++++XtWvXypUrVySc+Xw++zE2NtZ+NPsK0xu4eXswh6knTpwY1tuD75b10OPtt9+WuLg4mTlzphQVFcnVq1clmATdxUhv9fHHH0tXV5fEx8f7vW6ef/DBB+ImZqdaUlJi71xMd3rLli3y8MMPy5kzZ+xjwW5kwsfoa/vomeYW5vCbOdSUmpoq58+fl+9+97uSl5dn73iHDx8u4cZcOX/9+vUyf/58ewdrmN95RESExMTEuGZ76O5jPRhf+cpXZNKkSfYH1pqaGvn2t79tnyf69a9/LcEi6AMI/2N2Jj1mzZplB5LZwN555x155plnVJcN+p566qnef3/uc5+zt5HJkyfbvaKFCxdKuDHnQMyHLzecBw1kPaxevdpvezCDdMx2YD6cmO0iGAT9ITjTfTSf3m4dxWKeJyQkiJuZT3nTpk2T2tpacauebYDt43bmMK35+wnH7WPdunVy8OBBOXbsmN/tW8zv3By2b25udsX2sK6f9dAX84HVCKbtIegDyHSn58yZI+Xl5X5dTvM8MzNT3Kytrc3+NGM+2biVOdxkdiw3bx/mhlxmNJzbt48LFy7Y54DCafsw4y/MTre0tFSOHj1q//5vZvYVI0eO9NsezGEnc640nLYH6w7roS+nT5+2H4Nqe7BCwJ49e+xRTSUlJdZf//pXa/Xq1VZMTIzV1NRkucm3vvUt6/jx41ZdXZ31xz/+0crOzrbi4uLsETDhrLW11frzn/9sN7PJ/uQnP7H//a9//cue/uMf/9jeHg4cOGDV1NTYI8FSU1Ot//znP5Zb1oOZtnHjRnukl9k+jhw5Yn3+85+3pk6danV0dFjhYu3atZbX67X/DhobG3vb1atXe+dZs2aNNXHiROvo0aPWyZMnrczMTLuFk7V3WA+1tbXWD37wA/v/b7YH87eRlpZmZWVlWcEkJALI+NnPfmZvVBEREfaw7OrqasttnnzySSsxMdFeBxMmTLCfmw0t3B07dsze4d7azLDjnqHYL774ohUfH29/UFm4cKF19uxZy03rwex4cnJyrHHjxtnDkCdNmmStWrUq7D6k9fX/N23nzp2985gPHt/85jetz3zmM9Y999xjLVmyxN45u2k91NfX22ETGxtr/01MmTLFev755y2fz2cFE27HAABQEfTngAAA4YkAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIBo+D+svKbkRZHz1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = to_device(img.unsqueeze(0), device)     # img tensor get conv to a batch (basically adds an extra dimension)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()\n",
    "\n",
    "# for a random image\n",
    "img, label = test_dataset[82]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Predicted:', predict_image(img, model),end='  ')\n",
    "print('Actual:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e89519",
   "metadata": {},
   "source": [
    "### Testing over entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ec11b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss = 0.0904, Testing Accuracy = 97.3047%\n"
     ]
    }
   ],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=batch_size*2, num_workers=4, pin_memory=True), device)\n",
    "result = evaluate(model, test_loader)\n",
    "print(\"Testing loss = {:.4f}, Testing Accuracy = {:.4f}%\".format(result['val_loss'], result['val_acc']*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5684587",
   "metadata": {},
   "source": [
    "### Save and Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e9f1e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights_fname = 'mnist_weights.pth'\n",
    "torch.save(model.state_dict(), saved_weights_fname)    # state_dict() returns an ordered dict of all the weights and biases of the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
